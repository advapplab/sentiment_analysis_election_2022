{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz45HXMij8xpvQ1O0tPMt9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/advapplab/sentiment_analysis_election_2022/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "suxgfv_5R80u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_token = \"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\""
      ],
      "metadata": {
        "id": "W_Zv03G3R83F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2\"\n",
        "headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "data = query(\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"source_sentence\": \"I'm very happy\",\n",
        "            \"sentences\":[\"I'm filled with happiness\", \"I'm happy\"]\n",
        "        }\n",
        "    })"
      ],
      "metadata": {
        "id": "Hp-Wesb_R85n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = list()\n",
        "\n",
        "sentence_list.append(\"Data scientists need to know the possible combinations of techniques and choose the one that will lead to the best results and metrics. Regarding NLP, there are five state-of-the-art tasks: machine translation, sentiment analysis, question answering, text generation and entity recognition. Google BERT is the current most widely used pre-trained model for NLP tasks.\")\n",
        "sentence_list.append(\"Professor introduced several NLP datasets and models. Glue benchmark is the most important among all. NLP techniques include GPT and BERT.\")\n",
        "sentence_list.append(\"Natural language processing has evolved with the emergences newly released datasets. Names of datasets are abbreviations of Institutes publishing them.\")\n",
        "\n",
        "sentence_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYm5OoKHR9gI",
        "outputId": "734efe9b-9d2b-4431-91ef-bd67b34fb994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data scientists need to know the possible combinations of techniques and choose the one that will lead to the best results and metrics. Regarding NLP, there are five state-of-the-art tasks: machine translation, sentiment analysis, question answering, text generation and entity recognition. Google BERT is the current most widely used pre-trained model for NLP tasks.',\n",
              " 'Professor introduced several NLP datasets and models. Glue benchmark is the most important among all. NLP techniques include GPT and BERT.',\n",
              " 'Natural language processing has evolved with the emergences newly released datasets. Names of datasets are abbreviations of Institutes publishing them.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "machine_summary = \"This paper provides an overview of the current Natural Language Processing (NLP) landscape, highlighting the five main tasks, the most widely used pre-trained models, and the most important datasets. The five tasks are machine translation, sentiment analysis, question answering, text generation, and entity recognition. Google BERT is the most widely used pre-trained model for NLP tasks. The most important datasets are the GLUE benchmark, developed by Stanford University, and other datasets developed by other research institutes such as SQuAD, SNLI, and MNLI.\""
      ],
      "metadata": {
        "id": "FXj0FiEPSQ4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inside_dict = dict()\n",
        "inside_dict[\"source_sentence\"] = machine_summary\n",
        "inside_dict[\"sentences\"] =sentence_list\n",
        "\n",
        "outside_dict = dict()\n",
        "outside_dict[\"inputs\"] = inside_dict"
      ],
      "metadata": {
        "id": "4lJRQTozTOqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outside_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cUaE9enTj8Y",
        "outputId": "b208bc0f-8a36-468d-eba9-e20cecc57c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'inputs': {'source_sentence': 'This paper provides an overview of the current Natural Language Processing (NLP) landscape, highlighting the five main tasks, the most widely used pre-trained models, and the most important datasets. The five tasks are machine translation, sentiment analysis, question answering, text generation, and entity recognition. Google BERT is the most widely used pre-trained model for NLP tasks. The most important datasets are the GLUE benchmark, developed by Stanford University, and other datasets developed by other research institutes such as SQuAD, SNLI, and MNLI.',\n",
              "  'sentences': ['Data scientists need to know the possible combinations of techniques and choose the one that will lead to the best results and metrics. Regarding NLP, there are five state-of-the-art tasks: machine translation, sentiment analysis, question answering, text generation and entity recognition. Google BERT is the current most widely used pre-trained model for NLP tasks.',\n",
              "   'Professor introduced several NLP datasets and models. Glue benchmark is the most important among all. NLP techniques include GPT and BERT.',\n",
              "   'Natural language processing has evolved with the emergences newly released datasets. Names of datasets are abbreviations of Institutes publishing them.']}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query(outside_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zT6wLC_UCd9",
        "outputId": "db8b859f-58e4-4e10-cee9-af6641220cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7743879556655884, 0.6224311590194702, 0.5726655125617981]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_IGul2NVkab",
        "outputId": "a0c9fabb-43eb-4be5-8027-c084f2ee830b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.25.0.tar.gz (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 679 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from openai) (4.1.1)\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.2.0.62-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.25.0-py3-none-any.whl size=55880 sha256=ef2afb9d4557689116d6a62db8480741d4cce0faec5a5d2b05fa4084329a5c88\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/de/db/e82770b480ec30fd4a6d67108744b9c52be167c04fcf4af7b5\n",
            "Successfully built openai\n",
            "Installing collected packages: pandas-stubs, openai\n",
            "Successfully installed openai-0.25.0 pandas-stubs-1.2.0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = \"Machine translation is to translate a sentence in source language to a different target language.\"\n",
        "s2 = \" Professor did a very great explanation for me who have barely basic coding knowledge, but sometime go too fast in the coding part and I don’t know where to get a link of your example code so I kinda lost.\"\n",
        "s3 = \" An exploration of different NLP benchmark datasets, tasks and models.\"\n",
        "s4 = \" NLP packages now are really versatile and allow us to do different things. The course reminded us to try in different methods as well.\"\n",
        "s5 = \" Turing test is the most interesting content since we have done this discussion in another class. We watched the movie imitation game and discussed about the concept of robot and human’s thinking\"\n",
        "s6 = \" Data scientists need to know the possible combinations of techniques and choose the one that will lead to the best results and metrics. Regarding NLP, there are five state-of-the-art tasks: machine translation, sentiment analysis, question answering, text generation and entity recognition. Google BERT is the current most widely used pre-trained model for NLP tasks.\"\n",
        "s7 = \" Natural language processing has evolved with the emergences newly released datasets. Names of datasets are abbreviations of Institutes publishing them.\"\n",
        "s8 = \" Machine translation is to translate a sentence in source language to a different target language\"\n",
        "s9 = \" Professor did a very great explanation for me who have barely basic coding knowledge, but sometime go too fast in the coding part and I don’t know where to get a link of your example code so I kinda lost.\"\n",
        "s10 = \" Professor introduced a lot of NLP datasets and models explaining how they work and the techniques and benchmarks for it.\"\n",
        "s11 = \" NLP techniques that could generate answers, label texts, or identify language emotions.\"\n",
        "s12 = \" Sentimental analysis Task get the word and make it into tree bank which tells us which word is postove is negative.\"\n",
        "s13 = \" There are several ways to do NLP. But it looks like google BERT and GPT-3 are currently the leading models.\""
      ],
      "metadata": {
        "id": "nz4x3TsxXBj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=\"Summarize this for a second-grade student:\\n\\n\"+s1+s2+s3+s4+s5+s6+s7+s8+s9+s10+s11+s12+s13,\n",
        "  temperature=0.7,\n",
        "  max_tokens=1024,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")"
      ],
      "metadata": {
        "id": "KOUFsv5MUNlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "machine_summary = response[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "RPo_p5EJVdLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Summarize this for a second-grade student:\\n\\n\"+s1+s2+s3+s4+s5+s6+s7+s8+s9+s10+s11+s12+s13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "eN9BjfP57KaO",
        "outputId": "2be90a33-70f5-4814-8541-b6b7d6f32a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summarize this for a second-grade student:\\n\\nMachine translation is to translate a sentence in source language to a different target language. Professor did a very great explanation for me who have barely basic coding knowledge, but sometime go too fast in the coding part and I don’t know where to get a link of your example code so I kinda lost. An exploration of different NLP benchmark datasets, tasks and models. NLP packages now are really versatile and allow us to do different things. The course reminded us to try in different methods as well. Turing test is the most interesting content since we have done this discussion in another class. We watched the movie imitation game and discussed about the concept of robot and human’s thinking Data scientists need to know the possible combinations of techniques and choose the one that will lead to the best results and metrics. Regarding NLP, there are five state-of-the-art tasks: machine translation, sentiment analysis, question answering, text generation and entity recognition. Google BERT is the current most widely used pre-trained model for NLP tasks. Natural language processing has evolved with the emergences newly released datasets. Names of datasets are abbreviations of Institutes publishing them. Machine translation is to translate a sentence in source language to a different target language Professor did a very great explanation for me who have barely basic coding knowledge, but sometime go too fast in the coding part and I don’t know where to get a link of your example code so I kinda lost. Professor introduced a lot of NLP datasets and models explaining how they work and the techniques and benchmarks for it. NLP techniques that could generate answers, label texts, or identify language emotions. Sentimental analysis Task get the word and make it into tree bank which tells us which word is postove is negative. There are several ways to do NLP. But it looks like google BERT and GPT-3 are currently the leading models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnNbCU81YKvs",
        "outputId": "152c7797-2fc5-4ac1-9bf6-e45c3c9c3a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-6HnhrOSJ6nPIklfYOt2OUJR1JdHB2 at 0x7f2601110470> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \"\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1669700667,\n",
              "  \"id\": \"cmpl-6HnhrOSJ6nPIklfYOt2OUJR1JdHB2\",\n",
              "  \"model\": \"text-davinci-003\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 397,\n",
              "    \"total_tokens\": 397\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inside_dict = dict()\n",
        "inside_dict[\"source_sentence\"] = \"Data scientists use Natural Language Processing (NLP) to look at sentences, words and other pieces of a language and understand them better. They need to choose the best techniques to get the best results. One popular tool for NLP is Google BERT and some other important tasks are Machine Translation, Sentiment Analysis, Question Answering, Text Generation and Entity Recognition. Datasets and models help with understanding language better. Data scientists use Glue Benchmark, GPT and BERT to help them understand these datasets and models.\"\n",
        "inside_dict[\"sentences\"] = [s1, s2, s3, s4, s5, s6, s7, s9, s10, s11, s12, s13]\n",
        "\n",
        "outside_dict = dict()\n",
        "outside_dict[\"inputs\"] = inside_dict"
      ],
      "metadata": {
        "id": "xNQsZrcCWKEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = query(outside_dict)"
      ],
      "metadata": {
        "id": "pktaFOvvYGlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MOgcrB7_6SY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inside_dict[\"source_sentence\"]"
      ],
      "metadata": {
        "id": "suUhv3Kj9o78",
        "outputId": "9e560f9b-2e38-4f70-a9c3-4b99144c41cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Data scientists use Natural Language Processing (NLP) to look at sentences, words and other pieces of a language and understand them better. They need to choose the best techniques to get the best results. One popular tool for NLP is Google BERT and some other important tasks are Machine Translation, Sentiment Analysis, Question Answering, Text Generation and Entity Recognition. Datasets and models help with understanding language better. Data scientists use Glue Benchmark, GPT and BERT to help them understand these datasets and models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame()\n",
        "\n",
        "results['correlation'] = corr\n",
        "results['student_summaries'] = inside_dict[\"sentences\"]\n",
        "\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ERlF-iVJ82Vx",
        "outputId": "fa38e3e5-589f-425f-bbd7-ae45aef92e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    correlation                                  student_summaries\n",
              "0      0.421608  Machine translation is to translate a sentence...\n",
              "1      0.137622   Professor did a very great explanation for me...\n",
              "2      0.650878   An exploration of different NLP benchmark dat...\n",
              "3      0.521429   NLP packages now are really versatile and all...\n",
              "4      0.261990   Turing test is the most interesting content s...\n",
              "5      0.844187   Data scientists need to know the possible com...\n",
              "6      0.595005   Natural language processing has evolved with ...\n",
              "7      0.137622   Professor did a very great explanation for me...\n",
              "8      0.720811   Professor introduced a lot of NLP datasets an...\n",
              "9      0.584365   NLP techniques that could generate answers, l...\n",
              "10     0.254967   Sentimental analysis Task get the word and ma...\n",
              "11     0.697072   There are several ways to do NLP. But it look..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c79ef47-35c0-4f28-9d62-a1c2a59aa0c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>correlation</th>\n",
              "      <th>student_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.421608</td>\n",
              "      <td>Machine translation is to translate a sentence...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.137622</td>\n",
              "      <td>Professor did a very great explanation for me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.650878</td>\n",
              "      <td>An exploration of different NLP benchmark dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.521429</td>\n",
              "      <td>NLP packages now are really versatile and all...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.261990</td>\n",
              "      <td>Turing test is the most interesting content s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.844187</td>\n",
              "      <td>Data scientists need to know the possible com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.595005</td>\n",
              "      <td>Natural language processing has evolved with ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.137622</td>\n",
              "      <td>Professor did a very great explanation for me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.720811</td>\n",
              "      <td>Professor introduced a lot of NLP datasets an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.584365</td>\n",
              "      <td>NLP techniques that could generate answers, l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.254967</td>\n",
              "      <td>Sentimental analysis Task get the word and ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.697072</td>\n",
              "      <td>There are several ways to do NLP. But it look...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c79ef47-35c0-4f28-9d62-a1c2a59aa0c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c79ef47-35c0-4f28-9d62-a1c2a59aa0c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c79ef47-35c0-4f28-9d62-a1c2a59aa0c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GmAQvuQe8-ib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}